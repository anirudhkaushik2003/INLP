{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from conllu import parse\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchmetrics.classification import MulticlassF1Score as multiclass_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu\"\n",
    "dev_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu\"\n",
    "test_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu\"\n",
    "\n",
    "def read_conllu(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return parse(data)\n",
    "\n",
    "train_data = read_conllu(train_file)\n",
    "dev_data = read_conllu(dev_file)\n",
    "test_data = read_conllu(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(data):\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        sentence = [word['form'] for word in sentence]\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "train_sentences = get_sentences(train_data)\n",
    "dev_sentences = get_sentences(dev_data)\n",
    "test_sentences = get_sentences(test_data)\n",
    "\n",
    "\n",
    "tags_to_num = {\n",
    "    \"ADJ\": 0,\n",
    "    \"ADP\": 1,\n",
    "    \"ADV\": 2,\n",
    "    \"AUX\": 3,\n",
    "    \"CCONJ\": 4,\n",
    "    \"DET\": 5,\n",
    "    \"INTJ\": 6,\n",
    "    \"NOUN\": 7,\n",
    "    \"NUM\": 8,\n",
    "    \"PART\": 9,\n",
    "    \"PRON\": 10,\n",
    "    \"PROPN\": 11,\n",
    "    \"PUNCT\": 12,\n",
    "    \"SCONJ\": 13,\n",
    "    \"SYM\": 14,\n",
    "    \"VERB\": 15,\n",
    "    \"X\": 16,\n",
    "    \"PAD\": 17,\n",
    "}\n",
    "\n",
    "num_to_tag = {\n",
    "    0: \"ADJ\",\n",
    "    1: \"ADP\",\n",
    "    2: \"ADV\",\n",
    "    3: \"AUX\",\n",
    "    4: \"CCONJ\",\n",
    "    5: \"DET\",\n",
    "    6: \"INTJ\",\n",
    "    7: \"NOUN\",\n",
    "    8: \"NUM\",\n",
    "    9: \"PART\",\n",
    "    10: \"PRON\",\n",
    "    11: \"PROPN\",\n",
    "    12: \"PUNCT\",\n",
    "    13: \"SCONJ\",\n",
    "    14: \"SYM\",\n",
    "    15: \"VERB\",\n",
    "    16: \"X\",\n",
    "    17: \"PAD\",\n",
    "\n",
    "}\n",
    "\n",
    "def get_tags(data):\n",
    "    tags = []\n",
    "    for sentence in data:\n",
    "        sentence = [tags_to_num[word['upostag']] for word in sentence]\n",
    "        tags.append(sentence)\n",
    "    return tags\n",
    "\n",
    "train_tags = get_tags(train_data)\n",
    "dev_tags = get_tags(dev_data)\n",
    "test_tags = get_tags(test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "class POSDataset(Dataset):\n",
    "  def __init__(self, data: list[tuple[list[str], list[int]]], vocabulary:Vocab|None=None):\n",
    "    \"\"\"Initialize the dataset. Setup Code goes here\"\"\"\n",
    "    self.sentences = [i[0] for i in data]\n",
    "    self.labels = [i[1] for i in data]\n",
    "\n",
    "\n",
    "    if vocabulary is None:\n",
    "      self.vocabulary = build_vocab_from_iterator(self.sentences, specials=[START_TOKEN, END_TOKEN, UNKNOWN_TOKEN, PAD_TOKEN]) # use min_freq for handling unkown words better\n",
    "      self.vocabulary.set_default_index(self.vocabulary[UNKNOWN_TOKEN])\n",
    "    else:\n",
    "      # if vocabulary provided use that\n",
    "      self.vocabulary = vocabulary\n",
    "\n",
    "    self.sentences = []\n",
    "    self.labels = []\n",
    "    for j,(sentence, label) in enumerate(data):\n",
    "      sentence = [START_TOKEN] + sentence + [END_TOKEN]\n",
    "      label = [tags_to_num[\"PAD\"]] + label + [tags_to_num[\"PAD\"]]\n",
    "\n",
    "      # split into p+s+1 chunks\n",
    "      self.sentences.append(sentence)\n",
    "      self.labels.append(torch.nn.functional.one_hot(torch.tensor(label), num_classes=len(tags_to_num)))\n",
    "\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    \"\"\"Returns number of datapoints.\"\"\"\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Get the datapoint at `index`.\"\"\"\n",
    "    return torch.tensor(self.vocabulary.lookup_indices(self.sentences[index])), torch.tensor(self.labels[index])\n",
    "\n",
    "  def collate(self, batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Given a list of datapoints, batch them together\"\"\"\n",
    "    sentences = [i[0] for i in batch]\n",
    "    labels = [i[1] for i in batch]\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=self.vocabulary[PAD_TOKEN]) # pad sentences with pad token id\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=torch.tensor(tags_to_num[\"PAD\"])) # pad labels with 17\n",
    "\n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_POS_Tagger(nn.Module):\n",
    "    def __init__(self, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, 100)\n",
    "        self.lstm = nn.LSTM(100, 100, batch_first=True)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(100, 18),\n",
    "            nn.LogSoftmax(dim=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = POSDataset( list(zip(train_sentences, train_tags)))\n",
    "dev_dataset = POSDataset(list(zip(dev_sentences, dev_tags)), vocabulary=train_dataset.vocabulary)\n",
    "test_dataset = POSDataset(list(zip(test_sentences, test_tags)), vocabulary=train_dataset.vocabulary)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=train_dataset.collate, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=32, collate_fn=dev_dataset.collate)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, collate_fn=test_dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18396/1366289370.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.vocabulary.lookup_indices(self.sentences[index])), torch.tensor(self.labels[index])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 0 Loss: 60.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Accuracy: 0.934 Dev F1 Score: 0.602\n",
      "\n",
      "Epoch 1 Step 0 Loss: 5.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Accuracy: 0.975 Dev F1 Score: 0.854\n",
      "\n",
      "Epoch 2 Step 0 Loss: 2.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Accuracy: 0.981 Dev F1 Score: 0.871\n",
      "\n",
      "Epoch 3 Step 0 Loss: 1.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Accuracy: 0.983 Dev F1 Score: 0.880\n",
      "\n",
      "Epoch 4 Step 0 Loss: 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Accuracy: 0.982 Dev F1 Score: 0.882\n",
      "\n",
      "Epoch 5 Step 0 Loss: 0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Accuracy: 0.985 Dev F1 Score: 0.893\n",
      "\n",
      "Epoch 6 Step 0 Loss: 0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Accuracy: 0.986 Dev F1 Score: 0.896\n",
      "\n",
      "Epoch 7 Step 0 Loss: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Accuracy: 0.987 Dev F1 Score: 0.898\n",
      "\n",
      "Epoch 8 Step 0 Loss: 0.831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Accuracy: 0.987 Dev F1 Score: 0.898\n",
      "\n",
      "Epoch 9 Step 0 Loss: 0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n",
      "WARNING:root:Warning: Some classes do not exist in the target. F1 scores for these classes will be cast to zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Accuracy: 0.987 Dev F1 Score: 0.899\n",
      "\n",
      "\n",
      "Test Accuracy: 0.989 Test F1 Score: 0.962\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN_POS_Tagger(len(train_dataset.vocabulary))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# rnn = torch.nn.DataParallel(rnn)\n",
    "rnn.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(10):\n",
    "    for step, (word, tag) in enumerate(train_loader):\n",
    "        word, tag = word.to(device), tag.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = rnn(word)\n",
    "        tag = tag.float()\n",
    "        loss = 0\n",
    "        for i in range(tag.shape[1]):\n",
    "            loss += criterion(output[:,i,:], torch.argmax(tag[:,i,:], dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step%1000 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step} Loss: {loss.item():.3f}\")\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        dev_predictions = []\n",
    "        dev_labels = []\n",
    "        for word, tag in dev_loader:\n",
    "            word, tag = word.to(device), tag.to(device)\n",
    "            output = rnn(word)\n",
    "            output = torch.argmax(output, dim=2)\n",
    "            tag = torch.argmax(tag, dim=2)\n",
    "            for i in range(tag.shape[1]):\n",
    "                correct += (output[:,i] == tag[:,i]).sum()\n",
    "                dev_predictions.extend(output[:,i].tolist())\n",
    "                dev_labels.extend(tag[:,i].tolist())\n",
    "                total += tag.shape[0]\n",
    "\n",
    "        # caclulate f1 score\n",
    "        dev_f1_score = multiclass_f1_score(torch.tensor(dev_predictions), torch.tensor(dev_labels), num_classes=len(tags_to_num), average='macro')\n",
    "\n",
    "\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch {epoch} Accuracy: {correct/total:.3f} Dev F1 Score: {dev_f1_score:.3f}\")\n",
    "    print()\n",
    "        \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): \n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    for word, tag in test_loader:\n",
    "        word, tag = word.to(device), tag.to(device)\n",
    "        output = rnn(word)\n",
    "        output = torch.argmax(output, dim=2)\n",
    "        tag = torch.argmax(tag, dim=2)\n",
    "        for i in range(tag.shape[1]):\n",
    "            correct += (output[:,i] == tag[:,i]).sum()\n",
    "            test_predictions.extend(output[:,i].tolist())\n",
    "            test_labels.extend(tag[:,i].tolist())\n",
    "            total += tag.shape[0]\n",
    "\n",
    "    # caclulate f1 score\n",
    "    test_f1_score = multiclass_f1_score(torch.tensor(test_predictions), torch.tensor(test_labels),num_classes=len(tags_to_num), average='macro')\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"Test Accuracy: {correct/total:.3f} Test F1 Score: {test_f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> PAD PAD\n",
      "i PRON PRON\n",
      "want VERB VERB\n",
      "a DET DET\n",
      "flight NOUN NOUN\n",
      "from ADP ADP\n",
      "nashville PROPN PROPN\n",
      "to ADP ADP\n",
      "seattle PROPN PROPN\n",
      "that ADP ADP\n",
      "arrives VERB VERB\n",
      "no DET DET\n",
      "later ADJ ADJ\n",
      "than ADP ADP\n",
      "3 NUM NUM\n",
      "pm NOUN NOUN\n",
      "</s> PAD PAD\n"
     ]
    }
   ],
   "source": [
    "sentence = \"An apple a day keeps the doctor away\"\n",
    "sentence = sentence.lower()\n",
    "sentence = word_tokenize(sentence)\n",
    "sentence = test_sentences[1]\n",
    "sentence = [START_TOKEN] + sentence + [END_TOKEN]\n",
    "\n",
    "sentence = torch.tensor(train_dataset.vocabulary.lookup_indices(sentence)).to(device)\n",
    "# split into chunks of p+s+1\n",
    "tags = [tags_to_num[\"PAD\"]] + test_tags[1] + [tags_to_num[\"PAD\"]]\n",
    "output = rnn(sentence.unsqueeze(0))\n",
    "output = torch.argmax(output, dim=2)\n",
    "for i in range(output.shape[1]):\n",
    "    print(train_dataset.vocabulary.get_itos()[sentence[i].item()], num_to_tag[output[0,i].item()], num_to_tag[tags[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
