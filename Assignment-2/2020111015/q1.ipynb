{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from conllu import parse\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-train.conllu\"\n",
    "dev_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-dev.conllu\"\n",
    "test_file = \"./ud-treebanks-v2.13/UD_English-Atis/en_atis-ud-test.conllu\"\n",
    "\n",
    "def read_conllu(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return parse(data)\n",
    "\n",
    "train_data = read_conllu(train_file)\n",
    "dev_data = read_conllu(dev_file)\n",
    "test_data = read_conllu(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_sentences(data):\n",
    "    sentences = []\n",
    "    for sentence in data:\n",
    "        sentence = [word['form'] for word in sentence]\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "train_sentences = get_sentences(train_data)\n",
    "dev_sentences = get_sentences(dev_data)\n",
    "test_sentences = get_sentences(test_data)\n",
    "\n",
    "\n",
    "tags_to_num = {\n",
    "    \"ADJ\": 0,\n",
    "    \"ADP\": 1,\n",
    "    \"ADV\": 2,\n",
    "    \"AUX\": 3,\n",
    "    \"CCONJ\": 4,\n",
    "    \"DET\": 5,\n",
    "    \"INTJ\": 6,\n",
    "    \"NOUN\": 7,\n",
    "    \"NUM\": 8,\n",
    "    \"PART\": 9,\n",
    "    \"PRON\": 10,\n",
    "    \"PROPN\": 11,\n",
    "    \"PUNCT\": 12,\n",
    "    \"SCONJ\": 13,\n",
    "    \"SYM\": 14,\n",
    "    \"VERB\": 15,\n",
    "    \"X\": 16,\n",
    "    \"PAD\": 17,\n",
    "}\n",
    "\n",
    "num_to_tag = {\n",
    "    0: \"ADJ\",\n",
    "    1: \"ADP\",\n",
    "    2: \"ADV\",\n",
    "    3: \"AUX\",\n",
    "    4: \"CCONJ\",\n",
    "    5: \"DET\",\n",
    "    6: \"INTJ\",\n",
    "    7: \"NOUN\",\n",
    "    8: \"NUM\",\n",
    "    9: \"PART\",\n",
    "    10: \"PRON\",\n",
    "    11: \"PROPN\",\n",
    "    12: \"PUNCT\",\n",
    "    13: \"SCONJ\",\n",
    "    14: \"SYM\",\n",
    "    15: \"VERB\",\n",
    "    16: \"X\",\n",
    "    17: \"PAD\",\n",
    "\n",
    "}\n",
    "\n",
    "def get_tags(data):\n",
    "    tags = []\n",
    "    for sentence in data:\n",
    "        sentence = [tags_to_num[word['upostag']] for word in sentence]\n",
    "        tags.append(sentence)\n",
    "    return tags\n",
    "\n",
    "train_tags = get_tags(train_data)\n",
    "dev_tags = get_tags(dev_data)\n",
    "test_tags = get_tags(test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "\n",
    "class POSDataset(Dataset):\n",
    "  def __init__(self, p, s, data: list[tuple[list[str], list[int]]], vocabulary:Vocab|None=None):\n",
    "    \"\"\"Initialize the dataset. Setup Code goes here\"\"\"\n",
    "    self.p = p\n",
    "    self.s = s\n",
    "    self.sentences = [i[0] for i in data]\n",
    "    self.labels = [i[1] for i in data]\n",
    "\n",
    "\n",
    "    if vocabulary is None:\n",
    "      self.vocabulary = build_vocab_from_iterator(self.sentences, specials=[START_TOKEN, END_TOKEN, UNKNOWN_TOKEN, PAD_TOKEN]) # use min_freq for handling unkown words better\n",
    "      self.vocabulary.set_default_index(self.vocabulary[UNKNOWN_TOKEN])\n",
    "    else:\n",
    "      # if vocabulary provided use that\n",
    "      self.vocabulary = vocabulary\n",
    "\n",
    "    self.sentences = []\n",
    "    self.labels = []\n",
    "    for sentence, label in data:\n",
    "      sentence = [START_TOKEN] + sentence + [END_TOKEN]\n",
    "      label = [tags_to_num[\"PAD\"]] + label + [tags_to_num[\"PAD\"]]\n",
    "      sentence = [PAD_TOKEN] * (self.p) + sentence + [PAD_TOKEN] * (self.s)\n",
    "      label = [tags_to_num[\"PAD\"]] * (self.p) + label + [tags_to_num[\"PAD\"]] * (self.s)\n",
    "\n",
    "      # split into p+s+1 chunks\n",
    "      for i in range(self.p, len(sentence)-self.s-1):\n",
    "        temp = sentence[i-self.p:i+self.s+1]\n",
    "        self.sentences.append(temp)\n",
    "        self.labels.append(torch.nn.functional.one_hot(torch.tensor(label[i]), num_classes=len(tags_to_num)))\n",
    "        \n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    \"\"\"Returns number of datapoints.\"\"\"\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Get the datapoint at `index`.\"\"\"\n",
    "    return torch.tensor(self.vocabulary.lookup_indices(self.sentences[index])), torch.tensor(self.labels[index])\n",
    "\n",
    "  def collate(self, batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Given a list of datapoints, batch them together\"\"\"\n",
    "    sentences = [i[0] for i in batch]\n",
    "    labels = [i[1] for i in batch]\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=self.vocabulary[PAD_TOKEN]) # pad sentences with pad token id\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=torch.tensor(17)) # pad labels with 17\n",
    "\n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN_POS_Tagger(nn.Module):\n",
    "    def __init__(self, p, s, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.s = s\n",
    "\n",
    "        self.embedding_module = torch.nn.Embedding(vocabulary_size, 32)\n",
    "        self.entity_predictor = torch.nn.Sequential(\n",
    "                                    torch.nn.Linear(32, 20),\n",
    "                                    torch.nn.ReLU(),\n",
    "                                    torch.nn.Linear(20, 18))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_module(x)\n",
    "        x = self.entity_predictor(x)\n",
    "\n",
    "        # x = self.p i self.s\n",
    "        x = x[:, self.p, :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, s = 3, 2\n",
    "\n",
    "train_dataset = POSDataset(p,s, list(zip(train_sentences, train_tags)))\n",
    "dev_dataset = POSDataset(p,s,list(zip(dev_sentences, dev_tags)), vocabulary=train_dataset.vocabulary)\n",
    "test_dataset = POSDataset(p,s,list(zip(test_sentences, test_tags)), vocabulary=train_dataset.vocabulary)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6]) torch.Size([32, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26854/2039027421.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.vocabulary.lookup_indices(self.sentences[index])), torch.tensor(self.labels[index])\n"
     ]
    }
   ],
   "source": [
    "for i, j in train_loader:\n",
    "    print(i.shape, j.shape)\n",
    "    break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Step 0 Loss: 2.859\n",
      "Epoch 0 Step 1000 Loss: 0.501\n",
      "Epoch 0 Accuracy: 0.896\n",
      "Epoch 1 Step 0 Loss: 0.272\n",
      "Epoch 1 Step 1000 Loss: 0.097\n",
      "Epoch 1 Accuracy: 0.918\n",
      "Epoch 2 Step 0 Loss: 0.145\n",
      "Epoch 2 Step 1000 Loss: 0.353\n",
      "Epoch 2 Accuracy: 0.939\n",
      "Epoch 3 Step 0 Loss: 0.047\n",
      "Epoch 3 Step 1000 Loss: 0.136\n",
      "Epoch 3 Accuracy: 0.927\n",
      "Epoch 4 Step 0 Loss: 0.135\n",
      "Epoch 4 Step 1000 Loss: 0.274\n",
      "Epoch 4 Accuracy: 0.936\n",
      "Epoch 5 Step 0 Loss: 0.019\n",
      "Epoch 5 Step 1000 Loss: 0.012\n",
      "Epoch 5 Accuracy: 0.951\n",
      "Epoch 6 Step 0 Loss: 0.138\n",
      "Epoch 6 Step 1000 Loss: 0.233\n",
      "Epoch 6 Accuracy: 0.953\n",
      "Epoch 7 Step 0 Loss: 0.103\n",
      "Epoch 7 Step 1000 Loss: 0.040\n",
      "Epoch 7 Accuracy: 0.949\n",
      "Epoch 8 Step 0 Loss: 0.169\n",
      "Epoch 8 Step 1000 Loss: 0.270\n",
      "Epoch 8 Accuracy: 0.955\n",
      "Epoch 9 Step 0 Loss: 0.174\n",
      "Epoch 9 Step 1000 Loss: 0.557\n",
      "Epoch 9 Accuracy: 0.955\n",
      "\n",
      "Test Accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss() # use ignore index to ignore losses for padding value indices\n",
    "entity_predictor = FNN_POS_Tagger(p, s, len(train_dataset.vocabulary))\n",
    "optimizer = torch.optim.SGD(entity_predictor.parameters(), lr=1e-1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# entity_predictor = torch.nn.DataParallel(entity_predictor)\n",
    "entity_predictor.to(device)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    for step, (word, tag) in enumerate(train_loader):\n",
    "        word, tag = word.to(device), tag.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = entity_predictor(word)\n",
    "        tag = tag.float()\n",
    "        loss = loss_fn(output, tag)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step%1000 == 0:\n",
    "            print(f\"Epoch {epoch} Step {step} Loss: {loss.item():.3f}\")\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for word, tag in dev_loader:\n",
    "            word, tag = word.to(device), tag.to(device)\n",
    "            output = entity_predictor(word)\n",
    "            output = torch.argmax(output, dim=1)\n",
    "            tag = torch.argmax(tag, dim=1)\n",
    "            correct += (output == tag).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Epoch {epoch} Accuracy: {correct/len(dev_dataset):.3f}\")\n",
    "    print()\n",
    "        \n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): \n",
    "    for word, tag in test_loader:\n",
    "        word, tag = word.to(device), tag.to(device)\n",
    "        output = entity_predictor(word)\n",
    "        output = torch.argmax(output, dim=1)\n",
    "        tag = torch.argmax(tag, dim=1)\n",
    "        correct += (output == tag).sum()\n",
    "\n",
    "\n",
    "print()\n",
    "print(f\"Test Accuracy: {correct/len(test_dataset):.3f}\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD <s> PAD\n",
      "PRON what PRON\n",
      "AUX are AUX\n",
      "DET the DET\n",
      "NOUN coach NOUN\n",
      "NOUN flights NOUN\n",
      "ADP between ADP\n",
      "PROPN dallas PROPN\n",
      "CCONJ and CCONJ\n",
      "PROPN baltimore PROPN\n",
      "VERB leaving VERB\n",
      "NOUN august NOUN\n",
      "ADJ tenth ADJ\n",
      "CCONJ and CCONJ\n",
      "VERB returning VERB\n",
      "NOUN august NOUN\n",
      "VERB <unk> NUM\n",
      "PROPN </s> PAD\n"
     ]
    }
   ],
   "source": [
    "sentence = \"There were 70 children there.\"\n",
    "sentence = sentence.lower()\n",
    "sentence = word_tokenize(sentence)\n",
    "\n",
    "sentence = test_sentences[0]\n",
    "sentence = [START_TOKEN] + sentence + [END_TOKEN]\n",
    "sentence = [PAD_TOKEN] * p + sentence + [PAD_TOKEN] * s\n",
    "\n",
    "sentence = torch.tensor(train_dataset.vocabulary.lookup_indices(sentence)).to(device)\n",
    "# split into chunks of p+s+1\n",
    "chunks = [sentence[i:i+p+s+1] for i in range(len(sentence)-p-s)]\n",
    "\n",
    "tag_chunk = [tags_to_num[\"PAD\"]] + test_tags[0] + [tags_to_num[\"PAD\"]]\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    output = entity_predictor(chunk.unsqueeze(0))\n",
    "    output = torch.argmax(output, dim=1)\n",
    "    print(num_to_tag[output.item()], train_dataset.vocabulary.get_itos()[chunk[p].item()], num_to_tag[tag_chunk[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
